{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Structure-Constrained Resistivity Inversion\n\nThis example demonstrates how to incorporate structural information from \nseismic velocity models into ERT inversion for improved subsurface imaging.\n\nThe workflow includes:\n1. Loading seismic travel time data and performing velocity inversion\n2. Extracting velocity interfaces at specified thresholds\n3. Creating ERT meshes with geological layer boundaries\n4. Structure-constrained ERT inversion using velocity-derived interfaces\n5. Comparison with unconstrained inversion results\n\nStructure-constrained inversion significantly improves the accuracy of \nresistivity models by incorporating a priori geological information,\nleading to more reliable hydrological interpretations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport numpy as np\nimport pygimli as pg\nfrom pygimli.physics import ert\nimport pygimli.physics.traveltime as tt\nimport os\nimport sys\nimport matplotlib.pylab as pylab\n\n# Setup package path for development\ntry:\n    # For regular Python scripts\n    current_dir = os.path.dirname(os.path.abspath(__file__))\nexcept NameError:\n    # For Jupyter notebooks\n    current_dir = os.getcwd()\n\n# Add the parent directory to Python path\nparent_dir = os.path.dirname(current_dir)\nif parent_dir not in sys.path:\n    sys.path.append(parent_dir)\n\n# Import PyHydroGeophysX modules\nfrom PyHydroGeophysX.inversion.time_lapse import TimeLapseERTInversion\n\n# Set up matplotlib parameters\nparams = {'legend.fontsize': 15,\n         'axes.labelsize': 14,\n         'axes.titlesize':14,\n         'xtick.labelsize':14,\n         'ytick.labelsize':14}\npylab.rcParams.update(params)\nplt.rcParams[\"font.family\"] = \"Arial\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "output_dir = \"results/Structure_WC\"\nos.makedirs(output_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. load data\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "load seismic data\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ttData = tt.load(\"./results/workflow_example/synthetic_seismic_data.dat\")\n# load ERT data\nertData = ert.load(\"./results/TL_measurements/appres/synthetic_data30.dat\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "using ERT data to create a mesh to take care of the boundary\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "paraBoundary = 0.1\n\nert1 = ert.ERTManager(ertData)\ngrid = ert1.createMesh(data=ertData,quality = 31,paraDX=0.5, paraMaxCellSize=2, boundaryMaxCellSize=3000,smooth=[2, 2],\n                       paraBoundary = paraBoundary, paraDepth = 30.0)\nert1.setMesh(grid)\nmesh = ert1.fop.paraDomain\nmesh.setCellMarkers(np.ones((mesh.cellCount()))*2)\npg.show(mesh)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# travel time inversion\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "TT = pg.physics.traveltime.TravelTimeManager()\nTT.setMesh(mesh)\nTT.invert(ttData, lam=50,\n          zWeight=0.2,vTop=500, vBottom=5000,\n          verbose=1, limits=[100., 6000.])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ax, cbar = TT.showResult(cMap='jet',coverage=TT.standardizedCoverage(),cMin=500,cMax=5000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ax, cbar = TT.showResult(cMap='jet',cMin=500,cMax=5000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pg.show(TT.paraDomain,TT.model.array())\nprint(TT.paraDomain)\nprint(TT.model.array())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n\n# Assuming TT.model.array() gives you the velocity values\nvelocity_data = TT.model.array()\n\n# Get the mesh shape\nmesh = TT.paraDomain\ncell_centers = mesh.cellCenters()\nx_coords = cell_centers[:,0]  # X-coordinates of cell centers\nz_coords = cell_centers[:,1]  # Z-coordinates (depth) of cell centers\n\n# Create a new array for the thresholded values\nthresholded = np.ones_like(velocity_data, dtype=int)\n\n# Get unique x-coordinates (horizontal distances)\nunique_x = np.unique(x_coords)\n\n# For each vertical column (each unique x-coordinate)\nfor x in unique_x:\n    # Get indices of cells in this column, sorted by depth (z-coordinate)\n    column_indices = np.where(x_coords == x)[0]\n    column_indices = column_indices[np.argsort(z_coords[column_indices])]\n    \n    # Check if any cell in this column exceeds the threshold\n    threshold_crossed = False\n    \n    # Process cells from top to bottom\n    for idx in column_indices:\n        if velocity_data[idx] >= 1200 or threshold_crossed:\n            thresholded[idx] = 2\n            threshold_crossed = True\n        # Otherwise thresholded[idx] remains 1\n\n# Now thresholded contains your classified values (1 or 2)\n# Let's visualize the result\nimport matplotlib.pyplot as plt\n\n# Create a figure with two subplots to compare\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n\n# Original data\npg.show(mesh, velocity_data, ax=ax1, cMap='viridis', colorBar=True)\nax1.set_title('Original Velocity Data')\n\n# Thresholded data\npg.show(mesh, thresholded, ax=ax2, cMap='jet', colorBar=True)\nax2.set_title('Thresholded Data (1 & 2)')\n\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from scipy.interpolate import interp1d\n\ndef extract_velocity_interface(mesh, velocity_data, threshold=1200,interval = 4.0 ):\n    \"\"\"\n    Extract the interface where velocity equals the threshold value.\n    \n    Parameters:\n    mesh - The PyGIMLi mesh\n    velocity_data - The velocity values\n    threshold - The velocity value defining the interface (default: 1200)\n    \n    Returns:\n    x_dense, z_dense - Arrays with x and z coordinates of the smooth interface\n    \"\"\"\n    # Get cell centers\n    cell_centers = mesh.cellCenters()\n    x_coords = cell_centers[:,0]\n    z_coords = cell_centers[:,1]\n    \n    # Get x-range for complete boundary\n    x_min, x_max = np.min(x_coords), np.max(x_coords)\n    \n    # Create bins across the entire x-range\n     # Adjust for desired precision\n    x_bins = np.arange(x_min, x_max + interval, interval)\n    \n    # Arrays to store interface points\n    interface_x = []\n    interface_z = []\n    \n    # For each bin, find the velocity interface\n    for i in range(len(x_bins)-1):\n        # Get all cells in this x-range\n        bin_indices = np.where((x_coords >= x_bins[i]) & (x_coords < x_bins[i+1]))[0]\n        \n        if len(bin_indices) > 0:\n            # Get velocity values and depths for this bin\n            bin_velocities = velocity_data[bin_indices]\n            bin_depths = z_coords[bin_indices]\n            \n            # Sort by depth\n            sort_indices = np.argsort(bin_depths)\n            bin_velocities = bin_velocities[sort_indices]\n            bin_depths = bin_depths[sort_indices]\n            \n            # Find where velocity crosses the threshold\n            for j in range(1, len(bin_velocities)):\n                if (bin_velocities[j-1] < threshold and bin_velocities[j] >= threshold) or \\\n                   (bin_velocities[j-1] >= threshold and bin_velocities[j] < threshold):\n                    # Linear interpolation for exact interface depth\n                    v1 = bin_velocities[j-1]\n                    v2 = bin_velocities[j]\n                    z1 = bin_depths[j-1]\n                    z2 = bin_depths[j]\n                    \n                    # Calculate the interpolated z-value where velocity = threshold\n                    ratio = (threshold - v1) / (v2 - v1)\n                    interface_depth = z1 + ratio * (z2 - z1)\n                    \n                    interface_x.append((x_bins[i] + x_bins[i+1]) / 2)\n                    interface_z.append(interface_depth)\n                    break\n    \n    # Ensure we have interface points for the entire range\n    # If first point is missing, extrapolate from the first available points\n    if len(interface_x) > 0 and interface_x[0] > x_min + interval:\n        interface_x.insert(0, x_min)\n        # Use the slope of the first two points to extrapolate\n        if len(interface_x) > 2:\n            slope = (interface_z[1] - interface_z[0]) / (interface_x[1] - interface_x[0])\n            interface_z.insert(0, interface_z[0] - slope * (interface_x[1] - x_min))\n        else:\n            interface_z.insert(0, interface_z[0])\n    \n    # If last point is missing, extrapolate from the last available points\n    if len(interface_x) > 0 and interface_x[-1] < x_max - interval:\n        interface_x.append(x_max)\n        # Use the slope of the last two points to extrapolate\n        if len(interface_x) > 2:\n            slope = (interface_z[-1] - interface_z[-2]) / (interface_x[-1] - interface_x[-2])\n            interface_z.append(interface_z[-1] + slope * (x_max - interface_x[-1]))\n        else:\n            interface_z.append(interface_z[-1])\n    \n    # Create a dense interpolation grid for smoothing\n    x_dense = np.linspace(x_min, x_max, 500)  # 500 points for smooth curve\n    \n    # Apply cubic interpolation for smoother interface\n    if len(interface_x) > 3:\n        try:\n            interp_func = interp1d(interface_x, interface_z, kind='cubic', \n                                bounds_error=False, fill_value=\"extrapolate\")\n            z_dense = interp_func(x_dense)\n            \n            # Apply additional smoothing\n            from scipy.signal import savgol_filter\n            z_dense = savgol_filter(z_dense, window_length=31, polyorder=3)\n        except:\n            # Fall back to linear interpolation if cubic fails\n            interp_func = interp1d(interface_x, interface_z, kind='linear',\n                                bounds_error=False, fill_value=\"extrapolate\")\n            z_dense = interp_func(x_dense)\n    else:\n        # Not enough points for cubic interpolation\n        interp_func = interp1d(interface_x, interface_z, kind='linear',\n                               bounds_error=False, fill_value=\"extrapolate\")\n        z_dense = interp_func(x_dense)\n    \n\n\n    \n    return x_dense, z_dense"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Call the function with velocity data\nsmooth_x, smooth_z = extract_velocity_interface(mesh, velocity_data, threshold=1200,interval = 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ax1 = plt.subplots(1, 1, figsize=(12, 6))\n\n# Original data\npg.show(mesh, velocity_data, ax=ax1, cMap='viridis', colorBar=True)\nax1.set_title('Original Velocity Data')\nax1.plot(smooth_x, smooth_z)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "geo = pg.meshtools.createParaMeshPLC(ertData, quality=31, paraMaxCellSize=5,\n                                        paraBoundary=paraBoundary,paraDepth = 30.0,boundaryMaxCellSize=200)\n\npg.show(geo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def add_velocity_interface(ertData, smooth_x, smooth_z, paraBoundary=2, boundary=1):\n    \"\"\"\n    Add a velocity interface line to the geometry and create a mesh with different markers:\n    - Outside survey area: marker = 1\n    - Inside survey area, above velocity line: marker = 2\n    - Inside survey area, below velocity line: marker = 3\n    \n    Parameters:\n    ertData - ERT data with sensor positions\n    smooth_x, smooth_z - Arrays with x and z coordinates of the velocity interface\n    paraBoundary, boundary - Mesh parameters\n    \n    Returns:\n    markers - Array with cell markers\n    meshafter - The created mesh with updated markers\n    \"\"\"\n    # Create the initial parameter mesh\n    geo = pg.meshtools.createParaMeshPLC(ertData, quality=32, paraMaxCellSize=30,\n                                         paraBoundary=paraBoundary, paraDepth=30.0,\n                                         boundaryMaxCellSize=500)\n    \n    # Stack x and z coordinates for the interface\n    interface_points = np.vstack((smooth_x, smooth_z)).T\n    \n    # Extend the interface line beyond the data range by paraBoundary\n    input_points = np.vstack((\n        np.array([[interface_points[0][0] - paraBoundary, interface_points[0][1]]]),\n        interface_points,\n        np.array([[interface_points[-1][0] + paraBoundary, interface_points[-1][1]]])\n    ))\n    \n    # Create a polygon line for the interface\n    interface_line = pg.meshtools.createPolygon(input_points.tolist(), isClosed=False,\n                                              interpolate='linear', marker=99)\n    \n    # Add the interface to the geometry\n    geo_with_interface = geo + interface_line\n    \n    # Create a mesh from the combined geometry\n    meshafter = pg.meshtools.createMesh(geo_with_interface, quality=28)\n    \n    # Initialize all markers to 1 (outside region)\n    markers = np.ones(meshafter.cellCount())\n    \n    # Identify the survey area\n    survey_left = ertData.sensors()[0][0] - paraBoundary\n    survey_right = ertData.sensors()[-1][0] + paraBoundary\n    \n    # Process each cell\n    for i in range(meshafter.cellCount()):\n        cell_x = meshafter.cell(i).center().x()\n        cell_y = meshafter.cell(i).center().y()\n        \n        # Only modify markers within the survey area\n        if cell_x >= survey_left and cell_x <= survey_right:\n            # Interpolate the interface height at this x position\n            interface_y = np.interp(cell_x, input_points[:, 0], input_points[:, 1])\n            \n            # Set marker based on position relative to interface\n            if abs(cell_y) < abs(interface_y):\n                markers[i] = 2  # Below interface\n            else:\n                markers[i] = 3  # Above interface\n    \n    markers[meshafter.cellMarkers()==1] = 1 # Keep original markers for outside cells`\n    # Set the updated markers\n    meshafter.setCellMarkers(markers)\n    \n    return markers, meshafter\n\n# Example usage:\n# markers, meshafter = add_velocity_interface(ertData, smooth_x, smooth_z)\n# pg.show(meshafter, markers=True, label='Region markers')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "markers, mesh_with_interface = add_velocity_interface(ertData, smooth_x, smooth_z)\nmesh_with_interface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(10, 6))\npg.show(mesh_with_interface, markers, ax=ax, cMap='jet', colorBar=True)\nplt.title('Mesh with Velocity Interface')\nplt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ax, cbar = pg.show(mesh_with_interface)\nax.set_xlim([-10,120])\nax.set_ylim([1580,1630])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mesh_with_interface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mgrConstrained = ert.ERTManager()\nmgrConstrained.invert(data=ertData, verbose=True, lam=10, mesh=mesh_with_interface,limits=[1., 10000.])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from palettable.lightbartlein.diverging import BlueDarkRed18_18\nfixed_cmap = BlueDarkRed18_18.mpl_colormap\n\nres_cov = mgrConstrained.coverage()[mgrConstrained.paraDomain.cellMarkers()]>-1.2\n\nmgrConstrained.showResult(xlabel=\"Distance (m)\", ylabel=\"Elevation (m)\",coverage = res_cov,cMap=fixed_cmap)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mesh_with_interface.save(\"results/Structure_WC/mesh_with_interface.bms\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pg.show(mgrConstrained.paraDomain, mgrConstrained.coverage()[mgrConstrained.paraDomain.cellMarkers()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mgrConstrained.paraDomain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data_dir = \"results/TL_measurements/appres\"\n\n# List of ERT data files testing monthly time-lapse inversion\nert_files = [\n    \"synthetic_data30.dat\",\n    \"synthetic_data60.dat\",\n    \"synthetic_data90.dat\",\n    \"synthetic_data120.dat\",\n    \"synthetic_data150.dat\",\n    \"synthetic_data180.dat\",\n    \"synthetic_data210.dat\",\n    \"synthetic_data240.dat\",\n    \"synthetic_data270.dat\",\n    \"synthetic_data300.dat\",\n    \"synthetic_data330.dat\",\n    \"synthetic_data360.dat\",\n]\n\n# ert_files = [\n#     \"synthetic_data30.dat\",\n#     \"synthetic_data90.dat\",\n#     \"synthetic_data150.dat\",\n#     \"synthetic_data210.dat\",\n#     \"synthetic_data270.dat\",\n#     \"synthetic_data330.dat\",\n# ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pg.show(mesh_with_interface)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Full paths to data files\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data_files = [os.path.join(data_dir, f) for f in ert_files]\n\n# Measurement times (can be timestamps or any sequential numbers representing time)\nmeasurement_times = [1, 2, 3, 4, 5, 6, 7 ,8, 9, 10, 11, 12]  # Adjust based on your actual acquisition times\n\n# Create a mesh for the inversion (or load an existing one)\ndata = ert.load(data_files[0])\nert_manager = ert.ERTManager(data)\nmesh = ert_manager.createMesh(data=data, quality=34)\n\n# Set up inversion parameters\ninversion_params = {\n    \"lambda_val\": 50.0,              # Regularization parameter\n    \"alpha\": 10.0,                   # Temporal regularization parameter\n    \"decay_rate\": 0.0,               # Temporal decay rate\n    \"method\": \"cgls\",                # Solver method ('cgls', 'lsqr', etc.)\n    \"model_constraints\": (0.001, 1e4), # Min/max resistivity values (ohm-m)\n    \"max_iterations\": 15,            # Maximum iterations\n    \"absoluteUError\": 0.0,           # Absolute data error (V)\n    \"relativeError\": 0.05,           # Relative data error (5%)\n    \"lambda_rate\": 1.0,              # Lambda reduction rate\n    \"lambda_min\": 1.0,               # Minimum lambda value\n    \"inversion_type\": \"L2\"           # 'L1', 'L2', or 'L1L2'\n}\n\n# Create the time-lapse inversion object\ninversion = TimeLapseERTInversion(\n    data_files=data_files,\n    measurement_times=measurement_times,\n    mesh=mesh_with_interface,\n    **inversion_params\n)\n\n# Run the inversion\nprint(\"Starting time-lapse inversion...\")\nresult = inversion.run()\nprint(\"Inversion complete!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}